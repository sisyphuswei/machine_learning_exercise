{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1876ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20, current loss = 260.8554793592496, train accuracy = 92.41%\n",
      "epoch = 40, current loss = 101.79119865680313, train accuracy = 98.15%\n",
      "epoch = 60, current loss = 55.569389712859355, train accuracy = 99.44%\n",
      "epoch = 80, current loss = 33.421936228837936, train accuracy = 100.00%\n",
      "Totol time: 60.08s\n",
      "===============================Finish===================================\n"
     ]
    }
   ],
   "source": [
    "# simplest model, performing very poorly\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from utils_GRU import GRU_cell, softmax\n",
    "\n",
    "np.random.seed(7)\n",
    "tic = time.perf_counter()\n",
    "\n",
    "def accuracy(ytest, ypred):\n",
    "    return float(np.sum(np.all(ytest == ypred, axis=1))/ float(ytest.shape[0]))\n",
    "\n",
    "def accuracy_(ytest, ypred):\n",
    "    return float(np.sum(ytest == ypred)/ float(ytest.size))\n",
    "\n",
    "class seq2seq():\n",
    "    def __init__(self, X_shape, Y_shape, H=50, lr=0.0001):\n",
    "        \n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        # X is of shape (batch_size, seq_length, onehot_dim)\n",
    "        self.batch_size, self.n_encoder_cells, self.D = X_shape\n",
    "        _, self.n_decoder_cells, self.n_classes = Y_shape\n",
    "        # in this toy example, the onehot_dim is the n_classes\n",
    "        self.H = H \n",
    "\n",
    "        sos = np.zeros(self.n_classes)\n",
    "        sos[0] = 1\n",
    "        self.sos = np.zeros((self.batch_size, self.n_classes)) + sos\n",
    "\n",
    "        W = np.random.randn(self.H + self.D, self.H)\n",
    "        Wr = np.random.randn(self.H + self.D, self.H)\n",
    "        Wz = np.random.randn(self.H + self.D, self.H)\n",
    "        Wy = np.random.randn(self.H, self.n_classes)\n",
    "        by = np.zeros(self.n_classes)\n",
    "\n",
    "        self.Wy = Wy\n",
    "        self.by = by\n",
    "        self.h_init = np.random.randn(self.batch_size, self.H)\n",
    "\n",
    "        for i in range(self.n_encoder_cells):\n",
    "            self.encoder.append(GRU_cell(W, Wz, Wr, Wy, by))\n",
    "        for i in range(self.n_decoder_cells):\n",
    "            self.decoder.append(GRU_cell(W, Wz, Wr, Wy, by))\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.lb = preprocessing.LabelBinarizer()\n",
    "        self.lb.fit(np.arange(-1, self.n_classes-1)) # the n classes are <sos> 0, 1,...,n_classes-1\n",
    "\n",
    "    def feed_data(self, X, Y):\n",
    "        self.X = X \n",
    "        self.Y = Y\n",
    "\n",
    "    def feed_forward(self):\n",
    "        \"\"\"\n",
    "            Xt here is X[:, i, :] \n",
    "        \"\"\"\n",
    "        self.y_prob = np.zeros(self.Y.shape) \n",
    "        h = self.h_init\n",
    "        y = self.sos\n",
    "        for i, cell in enumerate(self.encoder):\n",
    "            h = cell.feed_forward(np.flip(self.X, axis=1)[:, i, :], h)\n",
    "        for i, cell in enumerate(self.decoder):\n",
    "            h = cell.feed_forward(y, h) \n",
    "            self.y_prob[:, i, :] = softmax(np.dot(h, self.Wy) + self.by) \n",
    "            y = self.lb.transform(np.argmax(self.y_prob[:, i, 1:], axis=1))\n",
    "            \n",
    "\n",
    "    def back_propagation(self):\n",
    "        \"\"\"\n",
    "        L = L_0 + ... + L_{n_decoder_cells}\n",
    "        \"\"\"\n",
    "        \n",
    "        dWy = 0\n",
    "        dby = 0\n",
    "        dh_last = 0 # here h_last is the final hidden vector of the encoder.\n",
    "\n",
    "        decoder_dW = [] # store all dLt/dW for any t\n",
    "        decoder_dWr = []\n",
    "        decoder_dWz = []\n",
    "\n",
    "        for i in np.arange(self.n_decoder_cells):\n",
    "            # dL_t/dW_hy = np.dot(h^t.T, y_prob[:, i, :] - y[:, i, :])\n",
    "            dLt_dWy = np.dot(self.decoder[i].h_output.T, self.y_prob[:, i, :] - self.Y[:, i, :])\n",
    "            dLt_dby = np.sum(self.y_prob[:, i, :] - self.Y[:, i, :], axis=0) \n",
    "            dWy += dLt_dWy\n",
    "            dby += dLt_dby\n",
    "\n",
    "            dLt_dht = np.dot(self.y_prob[:, i, :] - self.Y[:, i, :], self.Wy.T)\n",
    "            gradient = dLt_dht\n",
    "            for cell in reversed(self.decoder[:i+1]):\n",
    "                gradient = cell.back_propagation(gradient)\n",
    "            dh_last += gradient # gradient is indeed dLt/dh_last\n",
    "\n",
    "            dLt_dW = 0 # This is gonna be the graident dLt/dWh for the decoder \n",
    "            dLt_dWr = 0 \n",
    "            dLt_dWz = 0\n",
    "            for cell in self.decoder[:i+1]:\n",
    "                dLt_dW += cell.dW\n",
    "                dLt_dWr += cell.dWr \n",
    "                dLt_dWz += cell.dWz\n",
    "            decoder_dW.append(dLt_dW)\n",
    "            decoder_dWr.append(dLt_dWr)\n",
    "            decoder_dWz.append(dLt_dWz)\n",
    "            \n",
    "        dh = dh_last\n",
    "        # Now do the back propagation \n",
    "        for cell in reversed(self.encoder):\n",
    "            dh = cell.back_propagation(dh)\n",
    "\n",
    "        dW = sum(decoder_dW)  \n",
    "        dWr = sum(decoder_dWr)\n",
    "        dWz = sum(decoder_dWz)\n",
    "        # dL/dW_hx = dL/dh_0 dh_0/dW_hx + dL/dh_1 dh_1/dW_hx + ... + dL/dh_{n_cells-1} dh_{n_cells-1}/dW_hx\n",
    "        for cell in self.encoder:\n",
    "            dW += cell.dW\n",
    "            dWr += cell.dWr\n",
    "            dWz += cell.dWz\n",
    "\n",
    "        for cell in self.encoder:\n",
    "            cell.W -= self.lr * dW\n",
    "            cell.Wr -= self.lr * dWr \n",
    "            cell.Wz -= self.lr * dWz\n",
    "            cell.Wy -= self.lr * dWy\n",
    "            cell.by -= self.lr * dby\n",
    "\n",
    "        for cell in self.decoder:\n",
    "            cell.W -= self.lr * dW\n",
    "            cell.Wr -= self.lr * dWr \n",
    "            cell.Wz -= self.lr * dWz\n",
    "            cell.Wy -= self.lr * dWy\n",
    "            cell.by -= self.lr * dby\n",
    "\n",
    "\n",
    "    def predict(self, Xtest):\n",
    "        # Xtest has the same shape as that of X\n",
    "        y_prob = np.zeros(Xtest.shape) \n",
    "        h = self.h_init\n",
    "        y = self.sos\n",
    "        for i, cell in enumerate(self.encoder):\n",
    "            h = cell.feed_forward(np.flip(Xtest, axis=1)[:, i, :], h)\n",
    "        for i, cell in enumerate(self.decoder):\n",
    "            h = cell.feed_forward(y, h) \n",
    "            y_prob[:, i, :] = softmax(np.dot(h, self.Wy) + self.by) \n",
    "            y = self.lb.transform(np.argmax(self.y_prob[:, i, 1:], axis=1))\n",
    "        return np.argmax(y_prob[:, :, 1:], axis = 2) # The first column is always 0, it \n",
    "    # it represents the <sos>\n",
    "\n",
    "    def cross_entropy_loss(self):\n",
    "        \"\"\"\n",
    "            calculate loss after doing feed forward.\n",
    "        \"\"\"\n",
    "        # L = \\sum_t L_t \n",
    "        loss = 0\n",
    "        for i in np.arange(self.n_decoder_cells):\n",
    "            loss += -np.sum(self.Y[:, i, :]*np.log(self.y_prob[:, i, :] + 1e-6))\n",
    "        return loss\n",
    "\n",
    "\n",
    "X = np.loadtxt('X6.txt', delimiter= ' ').astype(int)\n",
    "idx = np.arange(216)\n",
    "np.random.shuffle(idx)\n",
    "X=X[idx]\n",
    "\n",
    "Y = (X+1)%3\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.arange(-1, 6))\n",
    "\n",
    "X_ohe = np.zeros((216, 3, 7))\n",
    "Y_ohe = np.zeros((216, 3, 7))\n",
    "for i in range(216):\n",
    "    X_ohe[i] = lb.transform(X[i])\n",
    "    Y_ohe[i] = lb.transform(Y[i])\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "toy_seq2seq = seq2seq(X_shape=(batch_size, 3, 7), Y_shape=(batch_size, 3, 7), H = 120, lr=0.0001)\n",
    "epochs = 80\n",
    "for i in range(epochs):\n",
    "    score = 0\n",
    "    loss = 0\n",
    "    for j in range(int(180/batch_size)):\n",
    "        toy_seq2seq.feed_data(X_ohe[batch_size*j:batch_size*(j+1)], Y_ohe[batch_size*j:batch_size*(j+1)])\n",
    "        toy_seq2seq.feed_forward()\n",
    "        toy_seq2seq.back_propagation()\n",
    "        y_pred = toy_seq2seq.predict(X_ohe[batch_size*j:batch_size*(j+1)])\n",
    "        score += accuracy_(y_pred, Y[batch_size*j:batch_size*(j+1)])\n",
    "        loss += toy_seq2seq.cross_entropy_loss()\n",
    "    score = score/int(180/batch_size)\n",
    "\n",
    "    if ((i + 1) % 20) == 0:\n",
    "        print('epoch = {}, current loss = {}, train accuracy = {:.2f}%'.format(i+1, loss, 100*score))\n",
    "        #print('epoch = {}, current loss = {}'.format(i+1, toy_seq2seq.cross_entropy_loss()))\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print('Totol time: {:.2f}s'.format(toc-tic))\n",
    "print('===============================Finish===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6d2d840c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_ohe = X_ohe[180:]\n",
    "Ytest = Y[180:]\n",
    "test_score = 0\n",
    "for i in range(36):\n",
    "    test_score+=accuracy_(toy_seq2seq.predict(Xtest_ohe[i*batch_size:(i+1)*batch_size]), Ytest[i*batch_size:(i+1)*batch_size])\n",
    "    #print(test_score)\n",
    "test_score/36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c1ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
