{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf51d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20, current loss = 654.6354406911717, train accuracy = 58.70%\n",
      "epoch = 40, current loss = 442.52578313574674, train accuracy = 69.44%\n",
      "epoch = 60, current loss = 382.85562757549997, train accuracy = 71.85%\n",
      "epoch = 80, current loss = 345.1368292507025, train accuracy = 73.52%\n",
      "epoch = 100, current loss = 317.26583164288934, train accuracy = 77.04%\n",
      "epoch = 120, current loss = 294.53033442283936, train accuracy = 79.26%\n",
      "epoch = 140, current loss = 274.4854622350271, train accuracy = 80.00%\n",
      "epoch = 160, current loss = 254.57884718993662, train accuracy = 81.48%\n",
      "epoch = 180, current loss = 234.77639008827794, train accuracy = 84.44%\n",
      "epoch = 200, current loss = 212.91580741708577, train accuracy = 87.22%\n",
      "epoch = 220, current loss = 190.43668778184426, train accuracy = 90.74%\n",
      "epoch = 240, current loss = 166.04552016215723, train accuracy = 92.04%\n",
      "epoch = 260, current loss = 143.79490227760144, train accuracy = 93.33%\n",
      "epoch = 280, current loss = 126.54791076751172, train accuracy = 94.63%\n",
      "epoch = 300, current loss = 112.28808770159365, train accuracy = 95.56%\n",
      "epoch = 320, current loss = 99.76081413048749, train accuracy = 96.48%\n",
      "epoch = 340, current loss = 88.13814188479249, train accuracy = 97.41%\n",
      "epoch = 360, current loss = 76.44658234736498, train accuracy = 98.70%\n",
      "epoch = 380, current loss = 65.33249616563415, train accuracy = 99.26%\n",
      "epoch = 400, current loss = 56.48882125140568, train accuracy = 99.26%\n",
      "epoch = 420, current loss = 49.37587128498128, train accuracy = 99.63%\n",
      "epoch = 440, current loss = 43.4218440886176, train accuracy = 99.63%\n",
      "epoch = 460, current loss = 38.66927801846114, train accuracy = 99.63%\n",
      "epoch = 480, current loss = 34.81582003182421, train accuracy = 99.63%\n",
      "epoch = 500, current loss = 31.59931472834584, train accuracy = 99.63%\n",
      "Totol time: 41.33s\n",
      "===============================Finish===================================\n"
     ]
    }
   ],
   "source": [
    "# simplest model, performing very poorly\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils import encoder_cell, decoder_cell, softmax\n",
    "\n",
    "np.random.seed(7)\n",
    "tic = time.perf_counter()\n",
    "\n",
    "def accuracy(ytest, ypred):\n",
    "    return float(np.sum(np.all(ytest == ypred, axis=1))/ float(ytest.shape[0]))\n",
    "\n",
    "def accuracy_(ytest, ypred):\n",
    "    \"\"\"\n",
    "    This function calculate how many right characters are correct.\n",
    "    \"\"\"\n",
    "    return float(np.sum(ytest == ypred)/ float(ytest.size))\n",
    "\n",
    "class seq2seq():\n",
    "    def __init__(self, X_shape, Y_shape, H=50, lr=0.0001):\n",
    "        \n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        # X is of shape (n_samples, seq_length, onehot_dim)\n",
    "        self.n_samples, self.n_encoder_cells, self.D = X_shape\n",
    "        _, self.n_decoder_cells, self.n_classes = Y_shape\n",
    "        # in this toy example, the onehot_dim is the n_classes\n",
    "        self.H = H \n",
    "\n",
    "        Wx = np.random.randn(self.D, self.H)\n",
    "        Wh = np.random.randn(self.H, self.H)\n",
    "        Wy = np.random.randn(self.H, self.n_classes)\n",
    "\n",
    "        self.Wy = Wy\n",
    "        self.h_init = np.random.randn(self.n_samples, self.H)\n",
    "\n",
    "        for i in range(self.n_encoder_cells):\n",
    "            self.encoder.append(encoder_cell(Wx, Wh))\n",
    "        for i in range(self.n_decoder_cells):\n",
    "            self.decoder.append(decoder_cell(Wh, Wy))\n",
    "        self.lr = lr\n",
    "\n",
    "    def feed_data(self, X, Y):\n",
    "        self.X = X \n",
    "        self.Y = Y\n",
    "\n",
    "    def feed_forward(self):\n",
    "        # input in reversed time order\n",
    "        self.y_prob = np.zeros(self.Y.shape) \n",
    "        h = self.h_init\n",
    "        for i, cell in enumerate(self.encoder):\n",
    "            h = cell.feed_forward(np.flip(self.X, axis=1)[:, i, :], h)\n",
    "        for i, cell in enumerate(self.decoder):\n",
    "            h = cell.feed_forward(h) \n",
    "            self.y_prob[:, i, :] = softmax(np.dot(h, self.Wy)) \n",
    "\n",
    "    def back_propagation(self):\n",
    "        \"\"\"\n",
    "        L = L_0 + ... + L_{n_decoder_cells}\n",
    "        \"\"\"\n",
    "        # dLt/dWy = np.dot(h^t.T, y_prob[:, i, :] - Y[:, i, :])\n",
    "        dWy = 0\n",
    "        dh_last = 0 # here h_last is the final hidden vector of the encoder.\n",
    "        decoder_dWh = []\n",
    "        for i in np.arange(self.n_decoder_cells):\n",
    "            \n",
    "            dLt_dWy = np.dot(self.decoder[i].h_output.T, self.y_prob[:, i, :] - self.Y[:, i, :])\n",
    "            dWy += dLt_dWy\n",
    "\n",
    "            dLt_dht = np.dot(self.y_prob[:, i, :] - self.Y[:, i, :], self.Wy.T)\n",
    "            gradient = dLt_dht\n",
    "            for cell in reversed(self.decoder[:i+1]):\n",
    "                gradient = cell.back_propagation(gradient)\n",
    "            dh_last += gradient # gradient is indeed dLt/dh_last\n",
    "\n",
    "            dLt_dWh = 0 # This is gonna be the graident dLt over dWh for the decoder \n",
    "            for cell in self.decoder[:i+1]:\n",
    "                dLt_dWh += cell.dWh\n",
    "            decoder_dWh.append(dLt_dWh)\n",
    "        \n",
    "        # dL/db_y = \\sum_axis=0 y_prob - y\n",
    "        #self.dL_dby = np.sum(self.y_prob - self.y, axis=0) \n",
    "        # dL/dh_last = np.dot(y_prob - y, W_hy.T)\n",
    "            \n",
    "        dh = dh_last\n",
    "        # do the backpropagation for the encoder\n",
    "        for cell in reversed(self.encoder):\n",
    "            dh = cell.back_propagation(dh)\n",
    "\n",
    "        dWx = 0\n",
    "        dWh = sum(decoder_dWh)  \n",
    "        for cell in self.encoder:\n",
    "            dWx += cell.dWx\n",
    "            dWh += cell.dWh\n",
    "\n",
    "        for cell in self.encoder:\n",
    "            cell.Wx -= self.lr * dWx\n",
    "            cell.Wh -= self.lr * dWh\n",
    "\n",
    "        for cell in self.decoder:\n",
    "            cell.Wh -= self.lr * dWh \n",
    "            cell.Wy -= self.lr * dWy\n",
    "\n",
    "    def predict(self, Xtest):\n",
    "        # Xtest has the same shape as that of X\n",
    "        y_prob = np.zeros(Xtest.shape) \n",
    "        h = self.h_init\n",
    "        for i, cell in enumerate(self.encoder):\n",
    "            h = cell.feed_forward(np.flip(Xtest, axis=1)[:, i, :], h)\n",
    "        for i, cell in enumerate(self.decoder):\n",
    "            h = cell.feed_forward(h) \n",
    "            y_prob[:, i, :] = softmax(np.dot(h, self.Wy))\n",
    "        return np.argmax(y_prob, axis = 2)\n",
    "\n",
    "    def cross_entropy_loss(self):\n",
    "        \"\"\"\n",
    "            calculate loss after doing feed forward.\n",
    "        \"\"\"\n",
    "        # L = \\sum_t Lt\n",
    "        # calculate y_prob\n",
    "        loss = 0\n",
    "        for i in np.arange(self.n_decoder_cells):\n",
    "            loss += -np.sum(self.Y[:, i, :]*np.log(self.y_prob[:, i, :] + 1e-6))\n",
    "        return loss\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = np.loadtxt('X6.txt', delimiter= ' ').astype(int)\n",
    "idx = np.arange(216)\n",
    "np.random.shuffle(idx)\n",
    "X=X[idx]\n",
    "\n",
    "#Y = np.flip(X, axis=1)\n",
    "Y = (X+1)%3\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(range(6))\n",
    "\n",
    "X_ohe = np.zeros((216, 3, 6))\n",
    "Y_ohe = np.zeros((216, 3, 6))\n",
    "for i in range(216):\n",
    "    X_ohe[i] = lb.transform(X[i])\n",
    "    Y_ohe[i] = lb.transform(Y[i])\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "toy_seq2seq = seq2seq(X_shape=(batch_size, 3, 6), Y_shape=(batch_size, 3, 6), H = 10, lr=0.0001)\n",
    "epochs = 500\n",
    "for i in range(epochs):\n",
    "    score = 0\n",
    "    loss = 0\n",
    "    for j in range(int(180/batch_size)):\n",
    "        toy_seq2seq.feed_data(X_ohe[batch_size*j:batch_size*(j+1)], Y_ohe[batch_size*j:batch_size*(j+1)])\n",
    "        toy_seq2seq.feed_forward()\n",
    "        toy_seq2seq.back_propagation()\n",
    "        y_pred = toy_seq2seq.predict(X_ohe[batch_size*j:batch_size*(j+1)])\n",
    "        score += accuracy_(y_pred, Y[batch_size*j:batch_size*(j+1)])\n",
    "        loss += toy_seq2seq.cross_entropy_loss()\n",
    "    score = score/int(180/batch_size)\n",
    "\n",
    "    if ((i + 1) % 20 == 0):\n",
    "        print('epoch = {}, current loss = {}, train accuracy = {:.2f}%'.format(i+1, loss, 100*score))\n",
    "        #print('epoch = {}, current loss = {}'.format(i+1, toy_seq2seq.cross_entropy_loss()))\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print('Totol time: {:.2f}s'.format(toc-tic))\n",
    "print('===============================Finish===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7422262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9629629629629631"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_ohe = X_ohe[180:]\n",
    "Ytest = Y[180:]\n",
    "test_score = 0\n",
    "for i in range(36):\n",
    "    test_score+=accuracy_(toy_seq2seq.predict(Xtest_ohe[i*batch_size:(i+1)*batch_size]), Ytest[i*batch_size:(i+1)*batch_size])\n",
    "    #print(test_score)\n",
    "test_score/36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "379d1fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1],\n",
       "       [0, 0, 1],\n",
       "       [2, 0, 1],\n",
       "       [2, 0, 0],\n",
       "       [2, 1, 2],\n",
       "       [1, 0, 2],\n",
       "       [1, 2, 0],\n",
       "       [2, 1, 2],\n",
       "       [2, 1, 2],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_seq2seq.predict(Xtest_ohe)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f18af9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 5, 4],\n",
       "       [3, 0, 4],\n",
       "       [5, 0, 1],\n",
       "       [5, 0, 3],\n",
       "       [4, 1, 2],\n",
       "       [1, 3, 2],\n",
       "       [1, 5, 3],\n",
       "       [1, 1, 2],\n",
       "       [5, 1, 2],\n",
       "       [0, 3, 1]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[180:190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb347b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
